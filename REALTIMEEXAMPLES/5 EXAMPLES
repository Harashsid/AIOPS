USECASE1:MONITORING STACK INTEGRATIONS
EXAMPLE1:
  I AM WORKING AS DEVOPS ENGINEER IN AN ECOMMER COMPANY FLIPKART.OUR TEAM DEPLOYED MICROSERVICES ON EKS PLATFORM.OUR TEAM INTERGRATED PROMETHEUS AND GRAFANA ,BUT LOGS AND ALERS ARE TOO NOISY.

  PROBLEM:
    10 + MICRO SERVICES-->EACH ONE SEND 50 ALERTS DAILY.
     HARD TO TRACE REAL ROOT CAUSE.
     EXAMPLE:PAYMENT SERVICE IS SLOW,BUT ALERT FROM INVENOTRY SERVICE.
     
  AIOPS SOLUTIONS:(WITH DYNATRACE)
    INSTALL ONE AGENT/SIDECAR ON ALL EKS PODS
    DYNTRACE CORELATES ALL TRACES,LOGS AND METRICS.
    AUTMATICALLY DETECTS"LATECNY IS DUE TO DB LOCK IN PAYMENT SERVICE"
         
  RESULT:
     -->INSTEAD OF 50 ALETS, WE GET ONE ALERTS
     -->INSTEAD OD DEBUGGING FOR 2 HOURS,WE PROCEDD WITH ONE HOUR.
USECASE 2:

STRUCTED TRACES  AND LOGS:
PROBLEM:
   WE ARE USING APPPLICATION WRITTEN IN NODEJS AND PYTHON WHIC HAS LOTS OF LOGS IN JSONLOGS,PLAIN TEXT,STACK TRACES.
   IF WE HAVE A PROBELM AT BACKEND, WE DO NOT KNOW WHERE IS THE SOURCE.
SOLUTION:
  DEVELOPERS USED OPEN ELEMENTRY TO INSTUMENTS TRCES.EACH REQUEST HAS A TRACID IN REQUEST
  FLUENTBIT FORWORD THE LOGS TO DATAGOG/LOKI  WHICH HAS LABELS
  GRAFANA ,WE CAN VIEW THE PATH.
RESULT:
  FIND THE FULLPATH TRACE:FRONTEND-->AUTH-->ORDER-->DB

USECASE3:
    SELF HEALING AUTOMATION
    PAYMENT PODS TAKING MORE THAN 90 PERCENTAGE,RESTART POD FOR EVERY 2 DAYS IS BORING
    MANUAL INTERVENTION IS NOT ADVISABLE
   AIOPS LIKE DYNATRACE/DATADOG,,CPU >90 PERCENTAGE,,RESTART POD
USECASE4:
     INCIDENT RESPONSE

   DEPLYMENT CAUSE LATENCY AND CREATED OUTAGE IN NEXT DAY.
   AIOPS TOOLS OBSERVES THAT LAST VERSION CAUSE DELAY AND GOING TO SLO FOR 30 MINUTES.
USECASE 5:
      RELEASE RISK SCROING.
    ONCE DEPLYMENT HAPPENED,UNTIL USER COMPLAINES WE HAVE NO IDEA ABOUT ISSUES.

   ONE BUILD DONE USING GITHUB ACTIONS, CALLED DYNATRACE APIS.
   IF ERROR COUNT > THRESHOLD,ROLLBACK THE DEPLOYMENT OR FAIL THE JOB


   


























  
    
